RNN:







Disadvantages: Vanishing Gradient Descent, Exploding GD




LSTM (Long Short Term Memory):

Forget Gate(Sigmoid) >> Input Gate(Sigmoid, Tanh) >> Output Gate(Tanh, Sigmoid).





GRU(Gated recurring Unit):

Here, forget and Input gate are combined. 

Hidden and memory cell are combined.



Good Explanation of LSTM:

https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47
